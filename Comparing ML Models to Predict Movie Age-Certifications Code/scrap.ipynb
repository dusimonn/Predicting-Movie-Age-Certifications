{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0.018164999999999997\n"
                }
            ],
            "source": [
                "num = [0.0098, 0.0241, 0.01, 0.0265, 0.0102, 0.0241, 0.0103, 0.0244, \n",
                "            0.0108, 0.0344, 0.0101, 0.0258, 0.0098, 0.0252, 0.0104, 0.0252, 0.0103, \n",
                "            0.0245, 0.0102, 0.0272]\n",
                "\n",
                "sum = 0\n",
                "for i in num:\n",
                "    sum += i\n",
                "print(sum / 20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0.05500000000000001\n0.024\n0.032\n0.174\n0.12100000000000002\n0.126\n0.491\n0.6620000000000001\n0.5549999999999999\n0.312\n0.21100000000000002\n0.24800000000000005\n"
                }
            ],
            "source": [
                "def find_ave(lst):\n",
                "    sum = 0\n",
                "    count = 0\n",
                "    for i in lst:\n",
                "        sum += i\n",
                "        count += 1\n",
                "    return sum / count\n",
                "\n",
                "G_precision = [0.00, 0.33, 0.00, 0.00, 0.00, 0.00, 0.22, 0.00, 0.00, 0.00]\n",
                "G_recall = [0.00, 0.10, 0.00, 0.00, 0.00, 0.00, 0.14, 0.00, 0.00, 0.00]\n",
                "G_f1_score = [0.00, 0.15, 0.00, 0.00, 0.00, 0.00, 0.17, 0.00, 0.00, 0.00]\n",
                "MA_precision = [0.00, 0.07, 0.23, 0.17, 0.34, 0.15, 0.21, 0.24, 0.16, 0.17]\n",
                "MA_recall = [0.00, 0.17, 0.17, 0.14, 0.16, 0.12, 0.11, 0.18, 0.10, 0.06]\n",
                "MA_f1_score = [0.00, 0.10, 0.19, 0.15, 0.12, 0.14, 0.14, 0.21, 0.12, 0.09]\n",
                "PG_precision = [0.50, 0.54, 0.54, 0.52, 0.46, 0.45, 0.42, 0.52, 0.46, 0.50]\n",
                "PG_recall = [0.63, 0.49, 0.77, 0.68, 0.69, 0.61, 0.69, 0.66, 0.66, 0.74]\n",
                "PG_f1_score = [0.56, 0.52, 0.64, 0.59, 0.52, 0.51, 0.52, 0.59, 0.51, 0.59]\n",
                "R_precision = [0.25, 0.29, 0.30, 0.31, 0.17, 0.36, 0.43, 0.40, 0.31, 0.30]\n",
                "R_recall = [0.22, 0.21, 0.14, 0.22, 0.03, 0.30, 0.27, 0.37, 0.17, 0.18]\n",
                "R_f1_score = [0.24, 0.24, 0.19, 0.26, 0.06, 0.33, 0.33, 0.39, 0.22, 0.22]\n",
                "\n",
                "everything = [G_precision, G_recall, G_f1_score, MA_precision, MA_recall, MA_f1_score, PG_precision, PG_recall, PG_f1_score, R_precision, R_recall, R_f1_score]\n",
                "for i in everything:\n",
                "    print(find_ave(i))\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": [
                "tit_unknown_dir = pd.read_csv(\"tit_unknown_dir.csv\")\n",
                "tit_unknown_dir_model_1 = pd.read_csv(\"tit_unknown_dir_model_1.csv\")\n",
                "tit_unknown_dir_model_2 = pd.read_csv(\"tit_unknown_dir_model_2.csv\")\n",
                "model_1_class = tit_unknown_dir_model_1[\"age_certification\"]\n",
                "model_2_class = tit_unknown_dir_model_2[\"age_certification\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model 1 counts {'G': 34, 'PG': 1675, 'MA': 179, 'R': 556}\nmodel 2 counts {'G': 82, 'PG': 850, 'MA': 598, 'R': 914}\ncommon predictions {'G': 0, 'PG': 597, 'MA': 25, 'R': 210}\ntotal_pred = 2444\ncommon_predictions = 832\ndifferent_predictions = 1612\n"
                }
            ],
            "source": [
                "# potentially usefully report metrics\n",
                "\n",
                "compare_class = pd.DataFrame({\"Model 1\": model_1_class, \"Model 2\": model_2_class})\n",
                "\n",
                "# return counts of each age cert for a model\n",
                "def cert_counts(model_class):\n",
                "    class_counts = {\"G\": 0, \"PG\": 0, \"MA\": 0, \"R\": 0}\n",
                "    for cert in list(model_class):\n",
                "        class_counts[cert] += 1\n",
                "    return class_counts\n",
                "\n",
                "model_1_counts = cert_counts(model_1_class)\n",
                "print(\"model 1 counts\", model_1_counts)\n",
                "model_2_counts = cert_counts(model_2_class)\n",
                "print(\"model 2 counts\", model_2_counts)\n",
                "\n",
                "# compare common predictions between models\n",
                "compare_class[\"compare\"] = model_1_class == model_2_class\n",
                "def common_counts(model_1_class, model_2_class):\n",
                "    class_counts = {\"G\": 0, \"PG\": 0, \"MA\": 0, \"R\": 0}\n",
                "    for i, comp_val in enumerate(compare_class[\"compare\"]):\n",
                "        if comp_val == True:\n",
                "            class_counts[model_1_class[i]] += 1\n",
                "    return class_counts\n",
                "\n",
                "common_pred = common_counts(model_1_class, model_2_class)\n",
                "print(\"common predictions\", common_pred)\n",
                "\n",
                "total_pred = sum(model_1_counts.values())\n",
                "common_count = sum(common_pred.values())\n",
                "print(\"total_pred =\", total_pred)\n",
                "print(\"common_predictions =\", common_count)\n",
                "print(\"different_predictions =\", total_pred-common_count)\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0.336\n0.21600000000000003\n0.254\n0.7040000000000001\n0.79\n0.742\n0.643\n0.643\n0.643\n0.653\n0.666\n0.6559999999999999\n0.9563\n"
                }
            ],
            "source": [
                "def find_ave(lst):\n",
                "    sum = 0\n",
                "    count = 0\n",
                "    for i in lst:\n",
                "        sum += i\n",
                "        count += 1\n",
                "    return sum / count\n",
                "\n",
                "G_precision = [.33, .38, .20, .21, .5, .33, .25, .33, .27, .56]\n",
                "G_recall = [.13, .3, .12, .23, .33, .28, .20, .08, .21, .28]\n",
                "G_f1_score = [.19, .33, .15, .22, .4, .3, .22, .12, .24, .37]\n",
                "MA_precision = [.63, .64, .66, .6, .78, .62, .74, .75, .86, .76]\n",
                "MA_recall = [.9, .7, .83, .82, .78, .75, .84, .83, .66, .79]\n",
                "MA_f1_score = [.75, .67, .74, .69, .78, .68, .79, .79, .75, .78]\n",
                "PG_precision = [.62, .69, .72, .65, .61, .62, .6, .65, .6, .67]\n",
                "PG_recall = [.58, .69, .67, .67, .6, .65, .59, .61, .66, .71]\n",
                "PG_f1_score = [.6, .69, .69, .66, .6, .64, .6, .63, .63, .69]\n",
                "R_precision = [.55, .65, .68, .74, .64, .73, .67, .59, .63, .65]\n",
                "R_recall = [.62, .65, .69, .59, .71, .67, .66, .72, .66, .69]\n",
                "R_f1_score = [.59, .65, .68, .65, .67, .7, .66, .65, .64, .67]\n",
                "variance_explained = [0.9563, 0.9563, 0.9563, 0.9563, 0.9563, 0.9563, 0.9563, 0.9563, 0.9563, 0.9563]\n",
                "\n",
                "everything = [G_precision, G_recall, G_f1_score, MA_precision, MA_recall, MA_f1_score, PG_precision, PG_recall, PG_f1_score, R_precision, R_recall, R_f1_score, variance_explained]\n",
                "for i in everything:\n",
                "    print(find_ave(i))\n",
                ""
            ]
        }
    ]
}
